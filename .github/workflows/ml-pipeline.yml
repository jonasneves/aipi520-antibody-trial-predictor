name: ML Pipeline
run-name: "ML Pipeline"

on:
  push:
    branches:
      - main
      - develop
    paths:
      - 'models/**'
      - 'scripts/**'
      - 'src/**'
      - 'requirements.txt'
      - 'run_pipeline.py'
      - '.github/workflows/ml-pipeline.yml'
  workflow_dispatch:

permissions:
  contents: write
  id-token: write

env:
  DATA_SOURCE: 'bulk_xml'
  CACHE_KEY_SUFFIX: 'bulk_xml'

jobs:
  # Stage 1: Data Preparation (Collection + Labeling)
  prepare-data:
    runs-on: ubuntu-latest
    timeout-minutes: 45  # Increased for bulk XML processing (~30 min for ~500K trials)
    outputs:
      cache-hit: ${{ steps.s3-cache.outputs.cache-hit }}
      s3-key: ${{ steps.s3-cache.outputs.s3-key }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: us-east-1

      - name: Check for cached raw data in S3
        id: s3-cache
        run: |
          FEATURE_KEY="features/antibody_phase23_${{ env.CACHE_KEY_SUFFIX }}.csv"
          RAW_KEY="raw_data/phase23_trials_${{ env.CACHE_KEY_SUFFIX }}.json"

          # Check for raw data
          if aws s3 ls "s3://aipi520-antibody-trial-predictor/$RAW_KEY" 2>/dev/null; then
            echo "raw-cache-hit=true" >> $GITHUB_OUTPUT
            echo "s3-key=$FEATURE_KEY" >> $GITHUB_OUTPUT
            echo "raw-s3-key=$RAW_KEY" >> $GITHUB_OUTPUT
            echo "Found cached raw data in S3, will skip API collection"
          else
            echo "raw-cache-hit=false" >> $GITHUB_OUTPUT
            echo "s3-key=$FEATURE_KEY" >> $GITHUB_OUTPUT
            echo "raw-s3-key=$RAW_KEY" >> $GITHUB_OUTPUT
            echo "No cache found, will collect data from API"
          fi

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Create data directory
        run: mkdir -p data

      - name: Download raw data from S3
        if: steps.s3-cache.outputs.raw-cache-hit == 'true'
        run: |
          aws s3 cp "s3://aipi520-antibody-trial-predictor/${{ steps.s3-cache.outputs.raw-s3-key }}" data/completed_phase2_3_trials_raw.json
          echo "Downloaded cached raw data from S3"

      - name: Convert cached bulk XML to CSV
        if: steps.s3-cache.outputs.raw-cache-hit == 'true'
        run: |
          echo "Converting cached raw JSON to CSV format..."
          python -c "
          import sys
          import json
          from pathlib import Path
          sys.path.insert(0, 'scripts')
          from parse_bulk_xml import convert_to_csv_format

          # Load raw JSON
          with open('data/completed_phase2_3_trials_raw.json', 'r') as f:
              trials = json.load(f)

          # Convert to CSV
          convert_to_csv_format(trials, Path('data/completed_phase2_3_trials.csv'))
          "
          echo "✓ Converted cached data to CSV"

      - name: Download and parse bulk XML from S3
        if: steps.s3-cache.outputs.raw-cache-hit != 'true'
        run: |
          echo "Downloading bulk XML file from S3 (~2.6GB, this may take several minutes)..."

          # Configure AWS CLI for better large file handling
          aws configure set default.s3.max_concurrent_requests 20
          aws configure set default.s3.multipart_threshold 64MB
          aws configure set default.s3.multipart_chunksize 16MB

          # Download with retry logic
          MAX_RETRIES=3
          RETRY_COUNT=0

          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            echo "Download attempt $((RETRY_COUNT + 1))/$MAX_RETRIES"

            if aws s3 cp "s3://aipi520-antibody-trial-predictor/bulk_data/ctg-public-xml.zip" data/ctg-public-xml.zip --no-progress; then
              echo "✓ Downloaded bulk XML from S3"
              break
            else
              RETRY_COUNT=$((RETRY_COUNT + 1))
              if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                echo "Download failed, retrying in 10 seconds..."
                sleep 10
              else
                echo "Error: Failed to download after $MAX_RETRIES attempts"
                exit 1
              fi
            fi
          done

          # Verify download
          if [ ! -f data/ctg-public-xml.zip ]; then
            echo "Error: Download file not found"
            exit 1
          fi

          FILE_SIZE=$(stat -f%z data/ctg-public-xml.zip 2>/dev/null || stat -c%s data/ctg-public-xml.zip)
          echo "Downloaded file size: $(($FILE_SIZE / 1024 / 1024)) MB"

          echo ""
          echo "Streaming and parsing XML files from ZIP archive..."
          echo "(No extraction needed - saves ~11GB disk space)"
          python scripts/parse_bulk_xml.py
          echo "✓ Parsed and filtered trials"

      - name: Upload raw data to S3
        if: steps.s3-cache.outputs.raw-cache-hit != 'true'
        run: |
          aws s3 cp data/completed_phase2_3_trials_raw.json "s3://aipi520-antibody-trial-predictor/${{ steps.s3-cache.outputs.raw-s3-key }}"
          echo "Uploaded raw data to S3 for future use"

      - name: Label data
        run: python run_pipeline.py --steps label

      - name: Create preparation summary
        run: |
          echo "# Data Preparation Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Collection stage summary
          echo "## Stage 1: Raw Data Collection" >> $GITHUB_STEP_SUMMARY
          if [ "${{ steps.s3-cache.outputs.raw-cache-hit }}" == "true" ]; then
            echo "**Source:** S3 Cache (skipped fresh download)" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**S3 Key:** \`${{ steps.s3-cache.outputs.raw-s3-key }}\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "**Source:** ClinicalTrials.gov Bulk XML (~500K trials parsed)" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          RAW_SAMPLES=$(tail -n +2 data/completed_phase2_3_trials.csv 2>/dev/null | wc -l || echo "N/A")
          RAW_COLS=$(head -1 data/completed_phase2_3_trials.csv 2>/dev/null | tr ',' '\n' | wc -l || echo "N/A")

          echo "**Output:** \`completed_phase2_3_trials.csv\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Trials:** $RAW_SAMPLES Phase 2/3 antibody trials" >> $GITHUB_STEP_SUMMARY
          echo "- **Columns:** ~$RAW_COLS (trial metadata, no labels yet)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Labeling stage summary
          echo "## Stage 2: Outcome Labeling" >> $GITHUB_STEP_SUMMARY
          echo "**Output:** \`clinical_trials_binary.csv\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          LABELED_SAMPLES=$(tail -n +2 data/clinical_trials_binary.csv | wc -l)
          LABELED_COLS=$(head -1 data/clinical_trials_binary.csv | tr ',' '\n' | wc -l)
          SUCCESS_COUNT=$(tail -n +2 data/clinical_trials_binary.csv | cut -d',' -f33 | grep -c '^1$' || echo "0")
          FAILURE_COUNT=$(tail -n +2 data/clinical_trials_binary.csv | cut -d',' -f33 | grep -c '^0$' || echo "0")
          SUCCESS_PCT=$(awk "BEGIN {printf \"%.1f\", ($SUCCESS_COUNT / $LABELED_SAMPLES) * 100}")
          FAILURE_PCT=$(awk "BEGIN {printf \"%.1f\", ($FAILURE_COUNT / $LABELED_SAMPLES) * 100}")

          echo "- **Samples:** $LABELED_SAMPLES labeled trials" >> $GITHUB_STEP_SUMMARY
          echo "- **Columns:** $LABELED_COLS (metadata + \`outcome_label\` + \`binary_outcome\`)" >> $GITHUB_STEP_SUMMARY
          echo "- **Success (1):** $SUCCESS_COUNT trials ($SUCCESS_PCT%)" >> $GITHUB_STEP_SUMMARY
          echo "- **Failure (0):** $FAILURE_COUNT trials ($FAILURE_PCT%)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "<details>" >> $GITHUB_STEP_SUMMARY
          echo "<summary>View Sample Data (first 3 rows)</summary>" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```csv' >> $GITHUB_STEP_SUMMARY
          head -4 data/clinical_trials_binary.csv >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "</details>" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "<details>" >> $GITHUB_STEP_SUMMARY
          echo "<summary>View All Columns ($LABELED_COLS total)</summary>" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          head -1 data/clinical_trials_binary.csv | tr ',' '\n' | nl >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "</details>" >> $GITHUB_STEP_SUMMARY

      - name: Upload raw labeled data
        uses: actions/upload-artifact@v4
        with:
          name: raw-labeled-data
          path: data
          retention-days: 1

  # Stage 1b (Parallel): EDA for Raw Data
  eda-raw:
    needs: prepare-data
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Download raw labeled data
        uses: actions/download-artifact@v4
        with:
          name: raw-labeled-data
          path: data

      - name: Generate EDA report for raw data
        run: |
          mkdir -p docs
          python scripts/generate_eda_raw.py \
            data/clinical_trials_binary.csv \
            docs/eda_raw_data.html \
            --timestamp "$(date -u +'%Y-%m-%d %H:%M:%S UTC')" \
            --data-source "ClinicalTrials.gov Bulk XML (~500K trials)" \
            --workflow-url "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

      - name: Upload EDA report
        uses: actions/upload-artifact@v4
        with:
          name: eda-raw-report
          path: docs/eda_raw_data.html
          retention-days: 30

  # Stage 2: Feature Engineering
  engineer-features:
    needs: prepare-data
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Create data directory
        run: mkdir -p data

      - name: Download raw labeled data
        uses: actions/download-artifact@v4
        with:
          name: raw-labeled-data
          path: data

      - name: Engineer features
        run: python run_pipeline.py --steps features

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: us-east-1

      - name: Upload features to S3
        run: |
          aws s3 cp data/clinical_trials_features.csv "s3://aipi520-antibody-trial-predictor/${{ needs.prepare-data.outputs.s3-key }}"
          echo "Uploaded features to S3 for archival"

      - name: Create engineering summary
        run: |
          echo "# Feature Engineering Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          SAMPLES=$(tail -n +2 data/clinical_trials_features.csv | wc -l)
          ENG_FEATURES=$(head -1 data/clinical_trials_features.csv | tr ',' '\n' | wc -l)

          echo "**Output:** \`clinical_trials_features.csv\` + \`clinical_trials_labels.csv\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Samples:** $SAMPLES trials (post-2024 filtered)" >> $GITHUB_STEP_SUMMARY
          echo "- **Features:** $ENG_FEATURES columns (labels saved separately)" >> $GITHUB_STEP_SUMMARY
          echo "- **Storage:** S3" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "<details>" >> $GITHUB_STEP_SUMMARY
          echo "<summary>View Sample Data (first 3 rows)</summary>" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```csv' >> $GITHUB_STEP_SUMMARY
          head -4 data/clinical_trials_features.csv >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "</details>" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "<details>" >> $GITHUB_STEP_SUMMARY
          echo "<summary>View All Engineered Features ($ENG_FEATURES total)</summary>" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          head -1 data/clinical_trials_features.csv | tr ',' '\n' | nl >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "</details>" >> $GITHUB_STEP_SUMMARY

      - name: Upload feature data
        uses: actions/upload-artifact@v4
        with:
          name: feature-data
          path: data
          retention-days: 1

  # Stage 2a (Parallel): EDA for Engineered Features
  eda-features:
    needs: engineer-features
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Download feature data
        uses: actions/download-artifact@v4
        with:
          name: feature-data
          path: data

      - name: Generate EDA report for features
        run: |
          mkdir -p docs
          python scripts/generate_eda_features.py \
            data/clinical_trials_features.csv \
            docs/eda_features.html \
            --timestamp "$(date -u +'%Y-%m-%d %H:%M:%S UTC')" \
            --data-source "ClinicalTrials.gov Bulk XML (~500K trials)" \
            --workflow-url "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

      - name: Upload EDA report
        uses: actions/upload-artifact@v4
        with:
          name: eda-features-report
          path: docs/eda_features.html
          retention-days: 30

  # Stage 2b: Train Models in Parallel (Matrix Strategy)
  train-models:
    needs: engineer-features
    runs-on: ubuntu-latest
    timeout-minutes: 20

    strategy:
      fail-fast: false
      matrix:
        model:
          - name: 'Logistic Regression'
            code: 'lr'
          - name: 'Decision Tree'
            code: 'dt'
          - name: 'Random Forest'
            code: 'rf'
          - name: 'Gradient Boosting'
            code: 'gb'
          - name: 'XGBoost'
            code: 'xgb'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Download feature data
        uses: actions/download-artifact@v4
        with:
          name: feature-data
          path: data

      - name: Train ${{ matrix.model.name }}
        run: |
          python scripts/train_single_model.py \
            ${{ matrix.model.code }} \
            data/clinical_trials_features.csv \
            models/

      - name: Create model training summary
        run: |
          echo "# ${{ matrix.model.name }} Training Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```json' >> $GITHUB_STEP_SUMMARY
          cat models/results_${{ matrix.model.code }}.json >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY

      - name: Upload model and results
        uses: actions/upload-artifact@v4
        with:
          name: model-${{ matrix.model.code }}
          path: |
            models/${{ matrix.model.code }}_model.pkl
            models/results_${{ matrix.model.code }}.json
          retention-days: 30

  # Stage 3: Aggregate Results and Compare
  aggregate-results:
    needs: [train-models, eda-raw, eda-features]
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Download all model results
        uses: actions/download-artifact@v4
        with:
          pattern: model-*
          merge-multiple: true
          path: models

      - name: Download EDA reports
        uses: actions/download-artifact@v4
        with:
          pattern: eda-*-report
          merge-multiple: true
          path: docs

      - name: Generate report (CSV + HTML)
        run: |
          mkdir -p reports docs
          python scripts/generate_overview.py models reports/model_comparison.csv docs/index.html \
            --timestamp "$(date -u +'%Y-%m-%d %H:%M:%S UTC')" \
            --workflow-url "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" \
            --repo "${{ github.repository }}"

      - name: Deploy dashboard and EDA reports
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./docs
          publish_branch: gh-pages
          commit_message: "Dashboard + EDA Reports - $(date -u +%Y-%m-%d)"

      - name: Commit results
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add models/*.pkl reports/model_comparison.csv docs/model_dashboard.html || true
          if ! git diff --staged --quiet; then
            git commit -m "Pipeline results - $(date -u +%Y-%m-%d)"
            git pull --rebase origin main
            git push
          fi

      - name: Create pipeline summary
        run: |
          echo "# ML Pipeline Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Configuration" >> $GITHUB_STEP_SUMMARY
          echo "- **Data Source:** ClinicalTrials.gov Bulk XML (~500K trials)" >> $GITHUB_STEP_SUMMARY
          echo "- **Models Trained:** 5 (in parallel)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "## Published Reports" >> $GITHUB_STEP_SUMMARY
          REPO_URL="https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}"
          echo "- [**Landing Page**](${REPO_URL}/)" >> $GITHUB_STEP_SUMMARY
          echo "- [Model Dashboard](${REPO_URL}/model_dashboard.html)" >> $GITHUB_STEP_SUMMARY
          echo "- [Raw Data EDA](${REPO_URL}/eda_raw_data.html)" >> $GITHUB_STEP_SUMMARY
          echo "- [Engineered Features EDA](${REPO_URL}/eda_features.html)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f reports/model_comparison.csv ]; then
            # Extract best model info using Python to properly handle quoted CSV fields
            BEST_INFO=$(python -c "import csv; f=open('reports/model_comparison.csv','r'); reader=csv.DictReader(f); row=next(reader); print(f\"{row['model_name']}|{row['roc_auc']}\"); f.close()")
            BEST_MODEL=$(echo "$BEST_INFO" | cut -d'|' -f1)
            BEST_AUC=$(echo "$BEST_INFO" | cut -d'|' -f2)

            echo "## Best Model" >> $GITHUB_STEP_SUMMARY
            echo "**${BEST_MODEL}** with ROC AUC: **${BEST_AUC}**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            echo "## Model Comparison" >> $GITHUB_STEP_SUMMARY
            echo '```csv' >> $GITHUB_STEP_SUMMARY
            cat reports/model_comparison.csv >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          else
            echo "WARNING: Model comparison report not found" >> $GITHUB_STEP_SUMMARY
          fi
