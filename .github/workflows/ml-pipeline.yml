name: ML Pipeline
run-name: "${{ github.event_name == 'workflow_dispatch' && (github.event.inputs.data_source == 'bulk_xml' && 'ML Pipeline - Bulk XML' || format('ML Pipeline - {0} studies', github.event.inputs.max_studies)) || format('ML Pipeline - {0}', github.ref_name) }}"

on:
  push:
    branches:
      - main
      - develop
    paths:
      - 'models/**'
      - 'scripts/**'
      - 'src/**'
      - 'requirements.txt'
      - 'run_pipeline.py'
      - '.github/workflows/ml-pipeline.yml'
  workflow_dispatch:
    inputs:
      data_source:
        description: 'Data source: api or bulk_xml'
        required: false
        default: 'api'
        type: choice
        options:
          - api
          - bulk_xml
      max_studies:
        description: 'Maximum trials to collect (0 = unlimited, only for API)'
        required: false
        default: '50000'
        type: string

permissions:
  contents: write
  id-token: write

env:
  DATA_SOURCE: ${{ github.event.inputs.data_source || 'api' }}
  MAX_STUDIES: ${{ github.event.inputs.max_studies || '50000' }}
  CACHE_KEY_SUFFIX: ${{ github.event.inputs.data_source == 'bulk_xml' && 'bulk_xml' || (github.event.inputs.max_studies == '0' && 'unlimited' || github.event.inputs.max_studies || '50000') }}

jobs:
  # Stage 1a: Data Collection & Labeling
  collect:
    runs-on: ubuntu-latest
    timeout-minutes: 45  # Increased for bulk XML processing (~30 min for ~500K trials)
    outputs:
      cache-hit: ${{ steps.s3-cache.outputs.cache-hit }}
      s3-key: ${{ steps.s3-cache.outputs.s3-key }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: us-east-1

      - name: Check for cached raw data in S3
        id: s3-cache
        run: |
          FEATURE_KEY="features/antibody_phase23_${{ env.CACHE_KEY_SUFFIX }}.csv"
          RAW_KEY="raw_data/phase23_trials_${{ env.CACHE_KEY_SUFFIX }}.json"

          # Check for raw data
          if aws s3 ls "s3://aipi520-antibody-trial-predictor/$RAW_KEY" 2>/dev/null; then
            echo "raw-cache-hit=true" >> $GITHUB_OUTPUT
            echo "s3-key=$FEATURE_KEY" >> $GITHUB_OUTPUT
            echo "raw-s3-key=$RAW_KEY" >> $GITHUB_OUTPUT
            echo "Found cached raw data in S3, will skip API collection"
          else
            echo "raw-cache-hit=false" >> $GITHUB_OUTPUT
            echo "s3-key=$FEATURE_KEY" >> $GITHUB_OUTPUT
            echo "raw-s3-key=$RAW_KEY" >> $GITHUB_OUTPUT
            echo "No cache found, will collect data from API"
          fi

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Create data directory
        run: mkdir -p data

      - name: Download raw data from S3
        if: steps.s3-cache.outputs.raw-cache-hit == 'true'
        run: |
          aws s3 cp "s3://aipi520-antibody-trial-predictor/${{ steps.s3-cache.outputs.raw-s3-key }}" data/completed_phase2_3_trials_raw.json
          echo "Downloaded cached raw data from S3"

      - name: Collect data from API
        if: steps.s3-cache.outputs.raw-cache-hit != 'true' && env.DATA_SOURCE == 'api'
        run: |
          python run_pipeline.py --steps collect --max-studies ${{ env.MAX_STUDIES }}

      - name: Download and parse bulk XML from S3
        if: steps.s3-cache.outputs.raw-cache-hit != 'true' && env.DATA_SOURCE == 'bulk_xml'
        run: |
          echo "Downloading bulk XML file from S3 (~4GB, this may take several minutes)..."
          aws s3 cp "s3://aipi520-antibody-trial-predictor/bulk_data/ctg-public-xml.zip" data/ctg-public-xml.zip
          echo "✓ Downloaded bulk XML from S3"
          echo ""
          echo "Streaming and parsing XML files from ZIP archive..."
          echo "(No extraction needed - saves ~11GB disk space)"
          python scripts/parse_bulk_xml.py
          echo "✓ Parsed and filtered trials"

      - name: Upload raw data to S3
        if: steps.s3-cache.outputs.raw-cache-hit != 'true'
        run: |
          aws s3 cp data/completed_phase2_3_trials_raw.json "s3://aipi520-antibody-trial-predictor/${{ steps.s3-cache.outputs.raw-s3-key }}"
          echo "Uploaded raw data to S3 for future use"

      - name: Label data
        run: python run_pipeline.py --steps label

      - name: Create collection summary
        run: |
          if [ "${{ steps.s3-cache.outputs.raw-cache-hit }}" == "true" ]; then
            echo "# Data Collection (Raw Data Cached)" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Raw data downloaded from S3 cache**" >> $GITHUB_STEP_SUMMARY
            echo "Skipped 10-minute API collection" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**S3 Key:** ${{ steps.s3-cache.outputs.raw-s3-key }}" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          echo "# Data Labeling Complete" >> $GITHUB_STEP_SUMMARY
          if [ "${{ steps.s3-cache.outputs.raw-cache-hit }}" != "true" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            if [ "${{ env.DATA_SOURCE }}" = "bulk_xml" ]; then
              echo "**Data Source:** ClinicalTrials.gov Bulk XML (parsed from ~500K trials)" >> $GITHUB_STEP_SUMMARY
            else
              echo "**Data Source:** ClinicalTrials.gov API (collected fresh)" >> $GITHUB_STEP_SUMMARY
            fi
          fi
          if [ "${{ env.MAX_STUDIES }}" = "0" ]; then
            echo "**Max Studies:** Unlimited (all available)" >> $GITHUB_STEP_SUMMARY
          else
            echo "**Max Studies:** ${{ env.MAX_STUDIES }}" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "## Generated Files" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          ls -lh data/clinical_trials_binary.csv >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          SAMPLES=$(tail -n +2 data/clinical_trials_binary.csv | wc -l)
          RAW_FEATURES=$(head -1 data/clinical_trials_binary.csv | tr ',' '\n' | wc -l)

          echo "**Samples:** $SAMPLES trials" >> $GITHUB_STEP_SUMMARY
          echo "**Raw Columns:** $RAW_FEATURES" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "<details>" >> $GITHUB_STEP_SUMMARY
          echo "<summary>View Sample Data (first 3 rows)</summary>" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```csv' >> $GITHUB_STEP_SUMMARY
          head -4 data/clinical_trials_binary.csv >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "</details>" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "<details>" >> $GITHUB_STEP_SUMMARY
          echo "<summary>View All Raw Columns ($RAW_FEATURES total)</summary>" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          head -1 data/clinical_trials_binary.csv | tr ',' '\n' | nl >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "</details>" >> $GITHUB_STEP_SUMMARY

      - name: Upload raw labeled data
        uses: actions/upload-artifact@v4
        with:
          name: raw-labeled-data
          path: data
          retention-days: 1

  # Stage 1b: Feature Engineering
  engineer:
    needs: collect
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Create data directory
        run: mkdir -p data

      - name: Download raw labeled data
        uses: actions/download-artifact@v4
        with:
          name: raw-labeled-data
          path: data

      - name: Engineer features
        run: python run_pipeline.py --steps features

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: us-east-1

      - name: Upload features to S3
        run: |
          aws s3 cp data/clinical_trials_features.csv "s3://aipi520-antibody-trial-predictor/${{ needs.collect.outputs.s3-key }}"
          echo "Uploaded features to S3 for archival"

      - name: Create engineering summary
        run: |
          echo "# Feature Engineering Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Uploaded to:** S3 for archival" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "## Generated Files" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          ls -lh data/clinical_trials_features.csv >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          SAMPLES=$(tail -n +2 data/clinical_trials_features.csv | wc -l)
          ENG_FEATURES=$(head -1 data/clinical_trials_features.csv | tr ',' '\n' | wc -l)

          echo "**Samples:** $SAMPLES trials" >> $GITHUB_STEP_SUMMARY
          echo "**Engineered Features:** $ENG_FEATURES columns" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "<details>" >> $GITHUB_STEP_SUMMARY
          echo "<summary>View Sample Data (first 3 rows)</summary>" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```csv' >> $GITHUB_STEP_SUMMARY
          head -4 data/clinical_trials_features.csv >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "</details>" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "<details>" >> $GITHUB_STEP_SUMMARY
          echo "<summary>View All Engineered Features ($ENG_FEATURES total)</summary>" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          head -1 data/clinical_trials_features.csv | tr ',' '\n' | nl >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "</details>" >> $GITHUB_STEP_SUMMARY

      - name: Upload feature data
        uses: actions/upload-artifact@v4
        with:
          name: feature-data
          path: data
          retention-days: 1

  # Stage 2: Train Models in Parallel (Matrix Strategy)
  train:
    needs: engineer
    runs-on: ubuntu-latest
    timeout-minutes: 20

    strategy:
      fail-fast: false
      matrix:
        model:
          - name: 'Logistic Regression'
            code: 'lr'
          - name: 'Decision Tree'
            code: 'dt'
          - name: 'Random Forest'
            code: 'rf'
          - name: 'Gradient Boosting'
            code: 'gb'
          - name: 'XGBoost'
            code: 'xgb'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Download feature data
        uses: actions/download-artifact@v4
        with:
          name: feature-data
          path: data

      - name: Train ${{ matrix.model.name }}
        run: |
          python scripts/train_single_model.py \
            ${{ matrix.model.code }} \
            data/clinical_trials_features.csv \
            models/

      - name: Create model training summary
        run: |
          echo "# ${{ matrix.model.name }} Training Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```json' >> $GITHUB_STEP_SUMMARY
          cat models/results_${{ matrix.model.code }}.json >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY

      - name: Upload model and results
        uses: actions/upload-artifact@v4
        with:
          name: model-${{ matrix.model.code }}
          path: |
            models/${{ matrix.model.code }}_model.pkl
            models/results_${{ matrix.model.code }}.json
          retention-days: 30

  # Stage 3: Aggregate Results and Compare
  aggregate:
    needs: train
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install pandas plotly

      - name: Download all model results
        uses: actions/download-artifact@v4
        with:
          pattern: model-*
          merge-multiple: true
          path: models

      - name: Generate report (CSV + HTML)
        run: |
          python scripts/generate_report.py models reports/model_comparison.csv docs/index.html \
            --timestamp "$(date -u +'%Y-%m-%d %H:%M:%S UTC')" \
            --workflow-url "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" \
            --repo "${{ github.repository }}"

      - name: Deploy dashboard
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./docs
          publish_branch: gh-pages
          commit_message: "Dashboard: ${{ env.MAX_STUDIES }} trials"

      - name: Commit results
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add models/*.pkl reports/model_comparison.csv docs/index.html || true
          if ! git diff --staged --quiet; then
            git commit -m "Pipeline results: ${{ env.MAX_STUDIES }} trials - $(date -u +%Y-%m-%d)"
            git push
          fi

      - name: Create pipeline summary
        run: |
          echo "# ML Pipeline Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Configuration" >> $GITHUB_STEP_SUMMARY
          if [ "${{ env.DATA_SOURCE }}" = "bulk_xml" ]; then
            echo "- **Data Source:** Bulk XML (~500K trials parsed)" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **Max Studies:** ${{ env.MAX_STUDIES }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Data Source:** ClinicalTrials.gov API" >> $GITHUB_STEP_SUMMARY
          fi
          echo "- **Models Trained:** 5 (in parallel)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f reports/model_comparison.csv ]; then
            # Extract best model info
            BEST_MODEL=$(tail -n +2 reports/model_comparison.csv | head -n 1 | cut -d',' -f1)
            BEST_AUC=$(tail -n +2 reports/model_comparison.csv | head -n 1 | cut -d',' -f6)

            echo "## Best Model" >> $GITHUB_STEP_SUMMARY
            echo "**${BEST_MODEL}** with ROC AUC: **${BEST_AUC}**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            echo "## Model Comparison" >> $GITHUB_STEP_SUMMARY
            echo '```csv' >> $GITHUB_STEP_SUMMARY
            cat reports/model_comparison.csv >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          else
            echo "WARNING: Model comparison report not found" >> $GITHUB_STEP_SUMMARY
          fi
